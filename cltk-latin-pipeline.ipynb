{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Text Analysis Pipeline for Ancient Languages with CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop will introduce the Classical Language Toolkit, an open-source Python framework dedicated to text analysis and natural language processing for historical languages. Participants will be taken through the basic stages of a text analysis pipeline, namely corpus loading, preprocessing, sentence and word tokenization, lemmatization, part-of-speech and morphological tagging, prosody identification, and more. Examples will given primarily using Latin texts (and English translations), though some attention will be given to other languages supported by the project, including Ancient Greek and Akkadian. Participants are encouraged to bring their laptops; the demonstration can be followed along with interactively in a web browser without any installation or setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint #RUN CELLS BY PRESSING SHIFT & RETURN TOGETHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with CLTK Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up corpora\n",
    "\n",
    "## You will need the models/datasets that the new lemmatizer uses\n",
    "## Note that this will generate an error if this and older \n",
    "## version of this corpus is already installed. If that happens,\n",
    "## backup the old version, move or delete it, and reimport the corpus.\n",
    "\n",
    "#from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "#corpus_importer = CorpusImporter('latin')\n",
    "#corpus_importer.list_corpora\n",
    "\n",
    "#corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "## We will be using the Latin Library corpus for today's workshop,\n",
    "## so we will also need to import that as well.\n",
    "\n",
    "#corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a CLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Latin Library corpus\n",
    "\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "ll = get_corpus_reader(language='latin', \n",
    "                       corpus_name='latin_text_latin_library')\n",
    "\n",
    "# The CLTK Latin Library corpus is a web-scraped collection of plaintext files\n",
    "# from thelatinlibrary.com.\n",
    "\n",
    "# We can access the individual files as follows...\n",
    "\n",
    "files = ll.fileids()\n",
    "print(files[:50]) # The first 50 files in the corpus\n",
    "\n",
    "# Note the [:50] slice to limit our list to the first 50 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "\n",
    "file_count = len(files)\n",
    "print(f'There are {file_count} files in this corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_files = [file for file in files if \"vergil\" in file]\n",
    "print(virgil_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ll.raw(virgil_files[0])[101:616])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ll.raw(virgil_files[0])[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ll.raw(virgil_files[0])[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = len(list(ll.words(virgil_files[0])))\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count = len(list(ll.sents(virgil_files[0])))\n",
    "print(sent_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for preprocessing\n",
    "\n",
    "import re # Regex module, useful for pattern matching\n",
    "import html # Useful for handling entities\n",
    "\n",
    "# Import/load a CLTK tool for normalizing i/j and u/v in Latin texts\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    # Remove Latin Library-specific paratexts with regex\n",
    "    \n",
    "    remove_list = [\n",
    "            r'\\bP. VERGILI MARONIS AENEIDOS LIBER .+\\b',\n",
    "            r'Vergil: Aeneid .+',\n",
    "            r'\\bThe Latin Library\\b',\n",
    "            r'\\bThe Classics Page\\b',\n",
    "            r'\\bVergil\\b',\n",
    "        ]\n",
    "    \n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove html entities and related html artifacts\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "    text = re.sub(r' \\xa0 ', '    ', text)\n",
    "    \n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize text\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    # Remove punctuation with translate\n",
    "    punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove numbers\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Handle spacing\n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\t',' ', text) # Remove tabs\n",
    "    text = re.sub('^\\s+','', text)\n",
    "    text = re.sub(' \\n', '\\n', text)\n",
    "    text = re.sub('\\n\\n', '~', text)\n",
    "    text = re.sub('~+', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_raw = ll.raw(virgil_files[0])\n",
    "print(virgil_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_pp = preprocess(ll.raw(virgil_files[0]))\n",
    "print(virgil_pp[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK Latin word tokenizer\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what the same poem looks like as a list of tokens\n",
    "\n",
    "tokens = word_tokenizer.tokenize(virgil_pp)\n",
    "\n",
    "print(tokens[:125])\n",
    "print('\\n')\n",
    "print(f'There are {len(tokens)} tokens in Virgil 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up CLTK Latin backoff lemmatizer\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = lemmatizer.lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmas[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lemmas[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS & Morphological Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "\n",
    "tagger = POSTag('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = tagger.tag_ngram_123_backoff(' '.join(tokens))\n",
    "pprint(pos_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags_2 = tagger.tag_tnt(' '.join(tokens))\n",
    "# pprint(pos_tags_2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosody Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import Levenshtein\n",
    "\n",
    "# from cltk.prosody.latin.hexameter_scanner import HexameterScanner\n",
    "# scanner = HexameterScanner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scansion = scanner.scan(virgil_pp[:44])\n",
    "# print(scansion.syllables)\n",
    "# print(scansion.scansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cltk.tokenize.line import LineTokenizer\n",
    "# line_tokenizer = LineTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = line_tokenizer.tokenize(virgil_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines[:10]:\n",
    "#     scansion = scanner.scan(line)\n",
    "#     print(scansion.scansion.replace(' ',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag import ner\n",
    "\n",
    "text_str = \"\"\"ut Venus, ut Sirius, ut Spica, ut aliae quae primae dicuntur esse mangitudinis.\"\"\"\n",
    "\n",
    "pprint(ner.tag_ner('latin', input_text=text_str, output_type=list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(text):\n",
    "\n",
    "    # Remove Latin Library-specific paratexts with regex\n",
    "    \n",
    "    remove_list = [\n",
    "            r'\\bP. VERGILI MARONIS AENEIDOS LIBER .+\\b',\n",
    "            r'Vergil: Aeneid .+',\n",
    "            r'\\bThe Latin Library\\b',\n",
    "            r'\\bThe Classics Page\\b',\n",
    "            r'\\bVergil\\b',\n",
    "        ]\n",
    "    \n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove html entities and related html artifacts\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "    text = re.sub(r' \\xa0 ', '    ', text)\n",
    "    \n",
    "    # Lowercase text\n",
    "#     text = text.lower()\n",
    "\n",
    "    # Normalize text\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    # Remove punctuation with translate\n",
    "    punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove numbers\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Handle spacing\n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\t',' ', text) # Remove tabs\n",
    "    text = re.sub('^\\s+','', text)\n",
    "    text = re.sub(' \\n', '\\n', text)\n",
    "    text = re.sub('\\n\\n', '~', text)\n",
    "    text = re.sub('~+', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_pp_2 = preprocess_2(ll.raw(virgil_files[0]))\n",
    "print(virgil_pp_2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ner.tag_ner('latin', input_text=virgil_pp_2, output_type=list)[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text Analyses & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(tokens)\n",
    "print(word_count.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = 0\n",
    "\n",
    "print('Top 25 words in Virgil 1:\\n')\n",
    "print(\"{number:>5}  {word:<12}{count:<12}{percent:<12}{running:<12}\". \\\n",
    "        format(number=\"\", word=\"TOKEN\", count=\"COUNT\", percent=\"TOKEN %\", running = \"RUNNING %\"))\n",
    "for i, pair in enumerate(word_count.most_common(25)):\n",
    "    running += pair[1]\n",
    "    print(\"{number:>5}. {word:<12}{count:<12}{percent:<12}{running:<12}\". \\\n",
    "        format(number=i+1, word=pair[0], count=pair[1], \\\n",
    "        percent=str(round(pair[1] / len(tokens)*100, 2))+\"%\", running = str(round(running / len(tokens)*100, 2))+\"%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_Text = Text(tokens)\n",
    "virgil_Text.concordance('Aeneas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virgil_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "virgil_Text.dispersion_plot(['aeneas', 'uenus', 'dido'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show books sorting problem\n",
    "\n",
    "aeneid_files = [file for file in files if \"vergil/aen\" in file]\n",
    "print(aeneid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix books sorting problem\n",
    "\n",
    "aeneid_order = [int(\" \".join(re.findall(r'\\d+', item))) for item in aeneid_files]\n",
    "aeneid_files = [x for _, x in sorted(zip(aeneid_order, aeneid_files))]\n",
    "print(aeneid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeneid_pp = preprocess(ll.raw(aeneid_files))\n",
    "aeneid_tokens = word_tokenizer.tokenize(aeneid_pp)\n",
    "aeneid_lemmas = [x for _, x in lemmatizer.lemmatize(aeneid_tokens)]\n",
    "\n",
    "# Error in lemmatizer; need to fix\n",
    "aeneid_lemmas = [lemma if lemma != 'dis-do' else 'dido' for lemma in aeneid_lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion plot of entire Aeneid\n",
    "\n",
    "aeneid_Text = Text(aeneid_tokens)\n",
    "plt.figure(figsize=(20, 5))\n",
    "aeneid_Text.dispersion_plot(['aeneas', 'uenus', 'dido'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized dispersion plot\n",
    "\n",
    "aeneid_Text = Text(aeneid_lemmas)\n",
    "plt.figure(figsize=(20, 5))\n",
    "aeneid_Text.dispersion_plot(['aeneas', 'uenus', 'dido'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphed Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(virgil_Text)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "fdist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with other CLTK Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Ancient Greek in CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/tesserae/tesserae/master/texts/grc/homer.iliad/homer.iliad.part.1.tess')\n",
    "iliad = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iliad[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iliad = re.sub(r'<.+?>\\t', '', iliad)\n",
    "print(iliad[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "iliad = unicodedata.normalize('NFC', iliad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "word_tokenizer_greek = WordTokenizer('greek')\n",
    "tokens = word_tokenizer_greek.tokenize(iliad)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.line import LineTokenizer\n",
    "\n",
    "line_tokenizer = LineTokenizer('greek')\n",
    "lines = line_tokenizer.tokenize(iliad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('greek')\n",
    "\n",
    "tagger.tag_ngram_123_backoff(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Akkadian in CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: import os\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "word_tokenizer_akkadian = WordTokenizer('akkadian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'u2-wa-a-ru at-ta e2-kal2-la-ka _e2_-ka wu-e-er'\n",
    "tokens = word_tokenizer_akkadian.tokenize(line)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.akkadian.syllabifier import Syllabifier\n",
    "\n",
    "word = \"epištašu\"\n",
    "syll = Syllabifier()\n",
    "syll.syllabify(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.akkadian.declension import NaiveDecliner\n",
    "\n",
    "word = 'ilum'\n",
    "decliner = NaiveDecliner()\n",
    "decliner.decline_noun(word, 'm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
