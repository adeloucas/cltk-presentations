{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Text Analysis Pipeline for Akkadian with CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop will introduce the Classical Language Toolkit, an open-source Python framework dedicated to text analysis and natural language processing for historical languages. Participants will be taken through the basic stages of a text analysis pipeline, namely corpus loading, preprocessing, sentence and word tokenization, lemmatization, part-of-speech and morphological tagging, prosody identification, and more. Examples will given primarily using Akkadian texts (and English translations), though some attention will be given to other languages supported by the project, including Latin and Ancient Greek. Participants are encouraged to bring their laptops; the demonstration can be followed along with interactively in a web browser without any installation or setup.\n",
    "\n",
    "Andrew Deloucas  \n",
    "Perseus/NEH Workshop  \n",
    "*Digital Editions and Digital Corpora*  \n",
    "Tufts University  \n",
    "5/31-6/1/19  \n",
    "  \n",
    "*Last updated 5/30/19*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cltk\n",
    "import os\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with CLTK Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up corpora\n",
    "\n",
    "## You will need the models/datasets that the new lemmatizer uses\n",
    "## Note that this will generate an error if this and older \n",
    "## version of this corpus is already installed. If that happens,\n",
    "## backup the old version, move or delete it, and reimport the corpus.\n",
    "##\n",
    "## The corpus we'll be using syncs with CDLI's github account, \n",
    "## which is backed-up daily. CDLI is the Cuneiform Digital Library \n",
    "## Initiative, who focus on digital publication of cuneiform writing.\n",
    "##\n",
    "## Their website is here: https://cdli.ucla.edu/\n",
    "##\n",
    "## To install on your own machine, uncomment this cell block and run the code below...\n",
    "\n",
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "# corpus_importer = CorpusImporter('akkadian')\n",
    "# print(corpus_importer.list_corpora)\n",
    "\n",
    "# corpus_importer.import_corpus('cdli_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a CLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and ensure that the Cuneiform Digital Library corpus is imported.\n",
    "\n",
    "parent_directory = os.path.expanduser('~')\n",
    "file = os.path.join(parent_directory, 'cltk_data', 'akkadian', \n",
    "                    'atf', 'cdli_corpus', 'cdliatf_unblocked.atf')\n",
    "os.path.isfile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your corpus. FileImport reads a .txt file and \n",
    "# saves to memory the text in .raw_file and .file_lines. \n",
    "# Parse_file captures information in a text file and formats \n",
    "# it in a clear, and disparate, manner for every text found.\n",
    "# \n",
    "# All of these inputs are 'invisible' and don't output \n",
    "# any information. \n",
    "\n",
    "from cltk.corpus.akkadian.file_importer import FileImport\n",
    "from cltk.corpus.akkadian.cdli_corpus import CDLICorpus\n",
    "\n",
    "fi = FileImport(file)\n",
    "cc = CDLICorpus()\n",
    "\n",
    "fi.read_file()\n",
    "cc.parse_file(fi.file_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examining available Metadata (unique to Akkadian).\n",
    "#\n",
    "# Depending on the information available from CDLI, you can\n",
    "# generate the availability of their data. This example \n",
    "# shows that you can filter through texts that have available\n",
    "# metadata, transliterations, normalizations and translation. \n",
    "# They're identified by their unique CDLI number (Pnum), \n",
    "# and edition (publication).\n",
    "#\n",
    "# Not all of our corpora has this feature, so it's a nice way to see how\n",
    "# Assyriologists divy up information for our texts.\n",
    "# \n",
    "# Using remote corpora like this is the most common method with which the \n",
    "# CLTK works, but we also have hosted corpora, such as Tesserae,\n",
    "# a collaborative between University at Buffalo, University of Norte Dame\n",
    "# and University of Geneva project which provides a web interface for \n",
    "# exploring intertextual parallels. The material there has proper \n",
    "# citation.bib, license.md, etc. files.\n",
    "#\n",
    "# Their homepage is here: http://tesserae.caset.buffalo.edu/\n",
    "# Version Control is here: https://github.com/cltk/greek_text_tesserae\n",
    "#\n",
    "# Note, I'm utilizing 'normalization' because it pulls the least amount \n",
    "# of data at once. Other filters include pnum, edition, metadata, \n",
    "# transliteration, normalization, and translation.\n",
    "\n",
    "cc.print_catalog(catalog_filter=['normalization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional tools for Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Catalog\n",
    "# This is the most memory intensive, as it grabs all \n",
    "# known information from every text: metadata, pnum,\n",
    "# edition, text, transliteration, normalization, and\n",
    "# translation.\n",
    "\n",
    "print('Catalog')\n",
    "pprint(list(cc.catalog)[:5])\n",
    "print()\n",
    "\n",
    "## Table of Contents\n",
    "# Shortened form of catalog: pnum, edition, and \n",
    "# line length of text.\n",
    "\n",
    "print('Table of Contents')        \n",
    "pprint(list(cc.toc())[:5])\n",
    "print()\n",
    "\n",
    "## List CDLI number / Edition\n",
    "# Shortest formation of this! Only pnums or editions.\n",
    "\n",
    "print('Edition')\n",
    "pprint(list(cc.list_editions())[:5]) \n",
    "print()\n",
    "print('CDLI Number')\n",
    "pprint(list(cc.list_pnums())[500:505])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line, String Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up CLTK Latin word tokenizer\n",
    "#\n",
    "# The Akkadian tokenizer reads ATF material and converts \n",
    "# data into readable, mutable tokens. There is an option\n",
    "# whether or not to preserve damage in the text. This type\n",
    "# of feature is something you'll never deal with using NLTK.\n",
    "# We'll get into how utilizing damage can affect our readings.\n",
    "#\n",
    "# For now, we'll keep it off.\n",
    "\n",
    "from cltk.corpus.akkadian.tokenizer import Tokenizer\n",
    "line_tokenizer = Tokenizer(preserve_damage=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select your text, we'll be choosing two:\n",
    "#\n",
    "# 1) a Tuppi Tamgurti (Legal Agreement text) from Nuzi\n",
    "# 2) Taylor's Prism (Royal Inscription of Sennacherib)\n",
    "\n",
    "nuzi = cc.catalog['P388524']['transliteration']\n",
    "sennacherib = cc.catalog['P462830']['transliteration']\n",
    "\n",
    "# Based on how CDLI structures its documents, we don't\n",
    "# need line tokenizers all the time; however, the option is \n",
    "# there in case you're working with other documentation.\n",
    "\n",
    "pprint(nuzi[0:8])\n",
    "print()\n",
    "pprint(sennacherib[0:8])\n",
    "print()\n",
    "print(f'There are {len(nuzi)} lines in Nuzi.')\n",
    "print(f'There are {len(sennacherib)} lines in Sennacherib.')\n",
    "\n",
    "# The first thing you may notice about our Akkadian lines:\n",
    "# sz = /sh/ phoneme, used to represent an 's-caron'.\n",
    "# commas = used to denote emphatic characters (t, and s,).\n",
    "# {curly bracket} = used to denote determinatives.\n",
    "# _underscores_ = used to denote Sumerian logograms.\n",
    "# numbers = denotes sign values (bi, bi2, bi3).\n",
    "# apostrophes = glottal stops\n",
    "# (parentheses) = numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nuzi-damage-sample'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of damage erasure lines 19, 20 of our Nuzi document:\n",
    "#\n",
    "# [square brackets] = missing signs\n",
    "# [...] = unrestorable\n",
    "# Pound sign = broken, but present\n",
    "\n",
    "pprint(nuzi[18:20])\n",
    "print()\n",
    "\n",
    "# Putting it through our line tokenizer.\n",
    "\n",
    "nuzi_sample = line_tokenizer.string_tokenizer('\\n'.join(nuzi[18:20]))\n",
    "pprint(nuzi_sample)\n",
    "\n",
    "# For those curious, these lines read:\n",
    "# \"If the woman Awishnaya ... the man Sheshwaya\n",
    "# after (lit. 'on the back of') the man Ari-peni ... the woman Shidanka\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word, Sign Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique to cuneiform languages, CLTK can additionally\n",
    "# tokenize texts by their word count and, further,\n",
    "# the cuneiform signs of which the words are composed.\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer('akkadian')\n",
    "\n",
    "# Word Tokenizers take strings, not lists.\n",
    "\n",
    "nuzi_words = word_tokenizer.tokenize('\\n'.join(nuzi[18:20]))\n",
    "sennacherib_words = word_tokenizer.tokenize('\\n'.join(sennacherib[0:1]))\n",
    "\n",
    "pprint(nuzi_words)\n",
    "print()\n",
    "pprint(sennacherib_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the tokenizers can differentiate between\n",
    "# phonetic Akkadian and logographic Sumerian writing\n",
    "# and we can confirm this with the sign tokenizer.\n",
    "\n",
    "nuzi_sign = [word_tokenizer.tokenize_sign(x) \n",
    "             for x in nuzi_words]\n",
    "sennacherib_sign = [word_tokenizer.tokenize_sign(x) \n",
    "                    for x in sennacherib_words]  \n",
    "  \n",
    "pprint(nuzi_sign[:5])\n",
    "print()\n",
    "\n",
    "# because the textual damage messes with our sign \n",
    "# readings, we'll have to turn it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuzi was already edited [above](#nuzi-damage-sample), so we'll tackle only Sennacherib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sennacherib_sample = line_tokenizer.string_tokenizer('\\n'.join(sennacherib))\n",
    "\n",
    "nuzi_fixed = word_tokenizer.tokenize('\\n'.join(nuzi_sample))\n",
    "sennacherib_fixed = word_tokenizer.tokenize('\\n'.join(sennacherib_sample[0:1]))\n",
    "\n",
    "nuzi_signs = [word_tokenizer.tokenize_sign(x) for x in nuzi_fixed]\n",
    "sennacherib_signs = [word_tokenizer.tokenize_sign(x) for x in sennacherib_fixed]  \n",
    "\n",
    "pprint(nuzi_signs)\n",
    "print()\n",
    "pprint(sennacherib_signs)\n",
    "\n",
    "# Here we can see some data trouble: Sennacherib's name is\n",
    "# spelled with sumerian logograms, but our data did not account\n",
    "# for this; our program isn't equipped to clean up data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further down the pipeline, we have a few smaller tests for normalized akkadian.\n",
    "\n",
    "## Syllabifier\n",
    "\n",
    "from cltk.stem.akkadian.syllabifier import Syllabifier\n",
    "\n",
    "word = \"epištašu\"\n",
    "syll = Syllabifier()\n",
    "syll.syllabify(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decliner\n",
    "\n",
    "from cltk.stem.akkadian.declension import NaiveDecliner\n",
    "\n",
    "word = 'ilum'\n",
    "decliner = NaiveDecliner()\n",
    "decliner.decline_noun(word, 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stresser\n",
    "\n",
    "from cltk.phonology.akkadian.stress import StressFinder\n",
    "stresser = StressFinder()\n",
    "word = \"šarrātim\"\n",
    "stresser.find_stress(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stems and Bound Forms\n",
    "\n",
    "from cltk.stem.akkadian.stem import Stemmer\n",
    "from cltk.stem.akkadian.bound_form import BoundForm\n",
    "stemmer = Stemmer()\n",
    "bound_former = BoundForm()\n",
    "s_word = \"ilātim\"\n",
    "b_word = \"kalbim\"\n",
    "stemmer.get_stem(word, 'f')\n",
    "bound_former.get_bound_form(word, 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consonant Vowel Parser\n",
    "\n",
    "from cltk.stem.akkadian.cv_pattern import CVPattern\n",
    "cv_patterner = CVPattern()\n",
    "word = \"iparras\"\n",
    "cv_patterner.get_cv_pattern(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text Analyses & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/de/Taylor_Prism-1.jpg\" alt=\"Taylor's Prism from the British Museum\" align=\"left\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Public domain photo by David Castor via [Wikimedia](https://upload.wikimedia.org/wikipedia/commons/d/de/Taylor_Prism-1.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.britishmuseum.org/collectionimages/AN01015/AN01015592_001_l.jpg\" alt=\"Detail, Taylor's Prism from the British Museum\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photo via [British Museum](https://www.britishmuseum.org/research/collection_online/collection_object_details/collection_image_gallery.aspx?partid=1&assetid=1015592001&objectid=295077) (BM 91032; CC BY-NC-SA 4.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.akkadian.atf_converter import ATFConverter\n",
    "from collections import Counter\n",
    "\n",
    "toto_signs = []\n",
    "\n",
    "lines = [line_tokenizer.string_tokenizer(text, include_blanks=False)\n",
    "        for text in nuzi]\n",
    "words = [word_tokenizer.tokenize(line[0]) for line in lines]\n",
    "\n",
    "for signs in words:\n",
    "    individual_words = [word_tokenizer.tokenize_sign(a) for a in signs]\n",
    "    individual_signs = [c for b in individual_words for c in b]\n",
    "    for count in individual_signs:\n",
    "        toto_signs.append(count)\n",
    "\n",
    "frequency_analysis = Counter(toto_signs).most_common(15)\n",
    "pprint(frequency_analysis)\n",
    "\n",
    "\n",
    "# We're not going to get too much information out of this, but for those\n",
    "# curious, dumu (4) means son or citizen, igi (5) can mean many\n",
    "# things, usually concerning eyes, sight, or witnesses. Munus (10) means\n",
    "# female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sennacherib_tokens = word_tokenizer.tokenize(\n",
    "    '\\n'.join(sennacherib_sample))\n",
    "s_tokens = [word[0] for word in sennacherib_tokens]\n",
    "word_count = Counter(s_tokens)\n",
    "\n",
    "running = 0\n",
    "\n",
    "print(\"Top 25 words in the Taylor's Prism:\\n\")\n",
    "print(\"{number:>5}  {word:<20}     {count:<12}{percent:<12}{running:<12}\". \\\n",
    "        format(number=\"\", word=\"TOKEN\", count=\"COUNT\", percent=\"TOKEN %\", running = \"RUNNING %\"))\n",
    "for i, pair in enumerate(word_count.most_common(25)):\n",
    "    running += pair[1]\n",
    "    print(\"{number:>5}. {word:<20}      {count:<12}{percent:<12}{running:<12}\". \\\n",
    "        format(number=i+1, word=pair[0], count=pair[1], \\\n",
    "        percent=str(round(pair[1] / len(s_tokens)*100, 2))+\"%\", running = str(round(running / len(s_tokens)*100, 2))+\"%\"))\n",
    "    \n",
    "# Majority of these words are particles or prepositional: \n",
    "# genitive (1, 2), subjunctive (2), accusative (3), \n",
    "# conjugational (4), and negative (5); prepositional (7,8,10)\n",
    "#\n",
    "# Unsurprisingly for a royal inscription, the top two \n",
    "# nouns are king (6), and the main deity of the Neo-Assyrian\n",
    "# Empire: Assur (9). \n",
    "# \n",
    "# Other words of note: campaign (11), types of land (12, 18)\n",
    "# and \"I counted, I surrounded\" (22, 23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sennacherib_Text = Text(s_tokens) # Note that Text takes a list of tokens as its input\n",
    "Sennacherib_Text.concordance('_lugal_')\n",
    "\n",
    "# The word 'king' can be seen before a few places:\n",
    "#\n",
    "#Lands\n",
    "#\n",
    "# Karduniash (Southern Mesopotamia, originally Kassite term)\n",
    "# Meluhha (Indus Valley)\n",
    "# Elam (modern Iran)\n",
    "# Babylonia (Southern Mesopotamia)\n",
    "#\n",
    "# Cities\n",
    "#\n",
    "# Siduni / Sidon (modern Lebanon)\n",
    "# Isqaluna / Ashkelon (modern Israel)\n",
    "# Asdudu / Ashdod (modern Israel)\n",
    "# Amqaruna / Ekron (modern Israel)\n",
    "# Hazitu / Gaza (modern Palestine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sennacherib_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "Sennacherib_Text.dispersion_plot(['_lugal_', 'asz-szur{ki}', '{kur}elam-ma{ki}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphed Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(Sennacherib_Text)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "fdist.plot(50, cumulative=True)\n",
    "\n",
    "# Our graph appears to follow Zipf's Law, in that the frequency of \n",
    "# any word is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with other CLTK Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Ancient Greek in CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/tesserae/tesserae/master/texts/grc/homer.iliad/homer.iliad.part.1.tess')\n",
    "iliad = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iliad[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iliad = re.sub(r'<.+?>\\t', '', iliad)\n",
    "print(iliad[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "iliad = unicodedata.normalize('NFC', iliad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "word_tokenizer_greek = WordTokenizer('greek')\n",
    "tokens = word_tokenizer_greek.tokenize(iliad)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.line import LineTokenizer\n",
    "\n",
    "line_tokenizer = LineTokenizer('greek')\n",
    "lines = line_tokenizer.tokenize(iliad)\n",
    "pprint(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta tool—let us know if you'd like to contribute to its development!\n",
    "\n",
    "from cltk.tag.pos import POSTag\n",
    "tagger = POSTag('greek')\n",
    "\n",
    "tagger.tag_ngram_123_backoff(lines[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
